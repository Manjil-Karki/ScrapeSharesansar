{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scrapeSharesansar.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**For educational purpose**"
      ],
      "metadata": {
        "id": "QkJE0JGIPZT3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Do not misuse!!**\n"
      ],
      "metadata": {
        "id": "p6fXMn3z8hGb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scraping https://www.sharesansar.com/ for price history as per your need"
      ],
      "metadata": {
        "id": "cXc1VPmtO2aB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver"
      ],
      "metadata": {
        "id": "ZJEcxZPRRfsO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e7381d8-151c-40cd-9c18-77fc9278b15d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.1.0-py3-none-any.whl (958 kB)\n",
            "\u001b[K     |████████████████████████████████| 958 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting urllib3[secure]~=1.26\n",
            "  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 49.9 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Collecting trio~=0.17\n",
            "  Downloading trio-0.19.0-py3-none-any.whl (356 kB)\n",
            "\u001b[K     |████████████████████████████████| 356 kB 14.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.4.0)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.0.0-py3-none-any.whl (24 kB)\n",
            "Collecting cryptography>=1.3.4\n",
            "  Downloading cryptography-36.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 42.9 MB/s \n",
            "\u001b[?25hCollecting pyOpenSSL>=0.14\n",
            "  Downloading pyOpenSSL-21.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure]~=1.26->selenium) (2021.10.8)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure]~=1.26->selenium) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure]~=1.26->selenium) (2.21)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from pyOpenSSL>=0.14->urllib3[secure]~=1.26->selenium) (1.15.0)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (3.10.0.2)\n",
            "Installing collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.8 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 cryptography-36.0.1 h11-0.13.0 outcome-1.1.0 pyOpenSSL-21.0.0 selenium-4.1.0 sniffio-1.2.0 trio-0.19.0 trio-websocket-0.9.2 urllib3-1.26.8 wsproto-1.0.0\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:11 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [76.0 kB]\n",
            "Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:15 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [872 kB]\n",
            "Hit:17 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,823 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,463 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,954 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,516 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [738 kB]\n",
            "Get:23 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [934 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,242 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [771 kB]\n",
            "Get:26 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]\n",
            "Fetched 14.7 MB in 4s (3,958 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 62 not upgraded.\n",
            "Need to get 95.3 MB of archives.\n",
            "After this operation, 327 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 97.0.4692.71-0ubuntu0.18.04.1 [1,142 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 97.0.4692.71-0ubuntu0.18.04.1 [84.7 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 97.0.4692.71-0ubuntu0.18.04.1 [4,370 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 97.0.4692.71-0ubuntu0.18.04.1 [5,055 kB]\n",
            "Fetched 95.3 MB in 5s (20.0 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 155229 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_97.0.4692.71-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (97.0.4692.71-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_97.0.4692.71-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (97.0.4692.71-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_97.0.4692.71-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (97.0.4692.71-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_97.0.4692.71-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (97.0.4692.71-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (97.0.4692.71-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (97.0.4692.71-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (97.0.4692.71-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (97.0.4692.71-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import Select\n",
        "import csv"
      ],
      "metadata": {
        "id": "1nnkx5vkRlzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing browser support\n",
        "\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "driver = webdriver.Chrome('chromedriver', options=chrome_options)"
      ],
      "metadata": {
        "id": "pK1konkuTR9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_URL = \"https://www.sharesansar.com/company-list\""
      ],
      "metadata": {
        "id": "ypXjA7KYUTJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "driver.get(BASE_URL)"
      ],
      "metadata": {
        "id": "6PNXUApmVflD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dr1 = driver"
      ],
      "metadata": {
        "id": "5HMLRRe2V6-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "select = Select(dr1.find_element(By.NAME, \"sector\"))\n",
        "# print(select.options)"
      ],
      "metadata": {
        "id": "ddXeBzvCEwuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "opts = []\n",
        "i = 0\n",
        "for opt in select.options:\n",
        "  opts.append(opt.text)\n",
        "  print(opt.text + '----->', i)\n",
        "  i = i + 1\n",
        "\n",
        "# selecting specific organization\n",
        "sel = int(input(\"Which is the sector you would like to scrape: \"))\n",
        "select.select_by_index(sel)\n",
        "dr1.find_element(By.ID, \"btn_listed_submit\").click()\n",
        "print(\"You selected\", opts[sel])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFfYq5XtHAEy",
        "outputId": "aa0c281f-4d48-422a-c799-23af52be7bc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Commercial Bank-----> 0\n",
            "Corporate Debentures-----> 1\n",
            "Development Bank-----> 2\n",
            "Finance-----> 3\n",
            "Government Bonds-----> 4\n",
            "Hotel & Tourism-----> 5\n",
            "Hydropower-----> 6\n",
            "Investment-----> 7\n",
            "Life Insurance-----> 8\n",
            "Manufacturing and Products-----> 9\n",
            "Microfinance-----> 10\n",
            "Mutual Fund-----> 11\n",
            "Non-Life Insurance-----> 12\n",
            "Others-----> 13\n",
            "Preference Share-----> 14\n",
            "Promoter Share-----> 15\n",
            "Trading-----> 16\n",
            "Which is the sector you would like to scrape: 0\n",
            "You selected Commercial Bank\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "time.sleep(5)\n",
        "organizations = {}\n",
        "\n",
        "\n",
        "while True:\n",
        "\n",
        "  soup = BeautifulSoup(dr1.page_source)\n",
        "  all_banks = soup.find(\"table\", attrs = {\"id\": \"myTable\"})\n",
        "  all_data = all_banks.find(\"tbody\").find_all(\"tr\")\n",
        "\n",
        "\n",
        "  for row in all_data:\n",
        "    table_data = row.find_all('td')\n",
        "    key = f\"{table_data[2].text} ({table_data[1].text})\"\n",
        "    val = table_data[1].find(\"a\")[\"href\"]\n",
        "\n",
        "    organizations[key] = val\n",
        "\n",
        "  try:\n",
        "    elem = dr1.find_element(By.ID, \"myTable_next\")\n",
        "    check = elem.get_attribute(\"class\").split(\" \")[-1]\n",
        "    if check == \"disabled\":\n",
        "      break\n",
        "    dr1.find_element(By.ID, \"myTable_next\").click()\n",
        "    time.sleep(5)\n",
        "  except:\n",
        "    break\n",
        "  \n",
        "print(f\"we have {len(organizations)} organizations\")\n",
        "time.sleep(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvJ54cOqt5OO",
        "outputId": "0ba44ea4-7454-466a-8f6f-b521c8d1da94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we have 26 organizations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"which {opts[sel]} organization do you like to scrape\")\n",
        "i = 0\n",
        "names = []\n",
        "for key in organizations:\n",
        "  print(key+\"--->\", i)\n",
        "  names.append(key)\n",
        "  i = i + 1\n",
        "\n",
        "sel = int(input(\"enter reference number: \"))\n",
        "print(\"you selected \", names[sel])\n",
        "key = names[sel]\n",
        "# print(key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FS3OSINJI4mO",
        "outputId": "5f848b58-b1c8-438b-e247-132eb90cc1ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "which Commercial Bank organization do you like to scrape\n",
            "NMB Bank Limited (NMB)---> 0\n",
            "Siddhartha Bank Limited (SBL)---> 1\n",
            "Nepal Credit & Commerce Bank Limited (NCCB)---> 2\n",
            "Kumari Bank Limited (KBL)---> 3\n",
            "Laxmi Bank Limited (LBL)---> 4\n",
            "Machhapuchchhre Bank Limited (MBL)---> 5\n",
            "Everest Bank Limited (EBL)---> 6\n",
            "Nepal Bangladesh Bank Limited (NBB)---> 7\n",
            "Nepal SBI Bank Limited (SBI)---> 8\n",
            "Himalayan Bank Limited (HBL)---> 9\n",
            "Standard Chartered Bank Nepal Limited (SCB)---> 10\n",
            "Nepal Investment Bank Limited (NIB)---> 11\n",
            "Nabil Bank Limited (NABIL)---> 12\n",
            "Citizens Bank International Limited (CZBIL)---> 13\n",
            "Prime Commercial Bank Limited (PCBL)---> 14\n",
            "Sunrise Bank Limited (SRBL)---> 15\n",
            "Agricultural Development Bank Limited (ADBL)---> 16\n",
            "Sanima Bank Limited (SANIMA)---> 17\n",
            "Mega Bank Nepal Limited (MEGA)---> 18\n",
            "Civil Bank Limited (CBL)---> 19\n",
            "Century Commercial Bank Limited (CCBL)---> 20\n",
            "Nepal Bank Limited (NBL)---> 21\n",
            "Global IME Bank Limited (GBIME)---> 22\n",
            "NIC Asia Bank Limited (NICA)---> 23\n",
            "Prabhu Bank Limited (PRVU)---> 24\n",
            "Bank of Kathmandu Limited (BOKL)---> 25\n",
            "enter reference number: 25\n",
            "you selected  Bank of Kathmandu Limited (BOKL)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dr1.get(organizations[key])\n",
        "\n",
        "\n",
        "dr1.find_element(By.ID, \"btn_cpricehistory\").click()\n",
        "\n",
        "\n",
        "select = Select(dr1.find_element(By.NAME, \"myTableCPriceHistory_length\"))\n",
        "select.select_by_index(2)\n",
        "\n",
        "run_once = 1\n",
        "while True:\n",
        "\n",
        "  time.sleep(2) \n",
        "  soup = BeautifulSoup(dr1.page_source)\n",
        "  table = soup.find(\"table\", attrs = {\"id\": \"myTableCPriceHistory\"})\n",
        "  \n",
        "  if run_once == 1:\n",
        "    headings = table.find(\"thead\").find_all(\"th\")\n",
        "    headings = [heading.text for heading in headings]\n",
        "    with open(key+\".csv\", 'w', newline='') as output_file:\n",
        "      dict_writer = csv.DictWriter(output_file, fieldnames = headings)\n",
        "      dict_writer.writeheader() \n",
        "    run_once = 0\n",
        "\n",
        "  table = table.find(\"tbody\").find_all(\"tr\")\n",
        "\n",
        "\n",
        "  table_data = []\n",
        "  for row in table:\n",
        "    data = row.find_all(\"td\")\n",
        "    data = [d.text for d in data]\n",
        "    table_data.append(dict(zip(headings, data)))\n",
        "    \n",
        "  print(f\"found {len(table_data)} rows in this page\")\n",
        "  \n",
        "  with open(key+\".csv\", 'a', newline='') as output_file:\n",
        "    dict_writer = csv.DictWriter(output_file, fieldnames = headings)   \n",
        "    dict_writer.writerows(table_data)\n",
        "\n",
        "  try:\n",
        "    dr1.find_element(By.ID, \"myTableCPriceHistory_next\").click()\n",
        "    print(\"scraping next page\")\n",
        "  except:\n",
        "    print(\"your work is done!!\")\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFz-YQxM1iZH",
        "outputId": "15f68b29-7c2a-4dc7-b56e-cbe43b481beb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 50 rows in this page\n",
            "scraping next page\n",
            "found 50 rows in this page\n",
            "scraping next page\n",
            "found 50 rows in this page\n",
            "scraping next page\n",
            "found 50 rows in this page\n",
            "scraping next page\n",
            "found 50 rows in this page\n",
            "scraping next page\n",
            "found 50 rows in this page\n",
            "scraping next page\n",
            "found 50 rows in this page\n",
            "scraping next page\n",
            "found 50 rows in this page\n",
            "scraping next page\n",
            "found 50 rows in this page\n",
            "scraping next page\n",
            "found 50 rows in this page\n",
            "scraping next page\n",
            "found 50 rows in this page\n",
            "scraping next page\n",
            "found 50 rows in this page\n",
            "scraping next page\n",
            "found 50 rows in this page\n",
            "scraping next page\n",
            "found 50 rows in this page\n",
            "scraping next page\n",
            "found 50 rows in this page\n",
            "scraping next page\n",
            "found 50 rows in this page\n",
            "scraping next page\n",
            "found 50 rows in this page\n",
            "scraping next page\n",
            "found 50 rows in this page\n",
            "scraping next page\n",
            "found 50 rows in this page\n",
            "scraping next page\n",
            "found 50 rows in this page\n",
            "scraping next page\n",
            "found 50 rows in this page\n",
            "scraping next page\n",
            "found 50 rows in this page\n",
            "scraping next page\n",
            "found 50 rows in this page\n",
            "scraping next page\n",
            "found 50 rows in this page\n",
            "scraping next page\n",
            "found 11 rows in this page\n",
            "your work is done!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GPZGywFNxu8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Fe5T9MVA74hi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}